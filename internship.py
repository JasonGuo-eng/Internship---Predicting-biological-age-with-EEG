# -*- coding: utf-8 -*-
"""Internship.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DNfb9nsQBc4CrBXTGJo5_vWhum68t9Bt

import data
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import shapiro, normaltest

# Example structure: each cell contains a 1D numpy array of length 180

wave_columns = ['delta','highAlpha', 'highBeta',
'lowAlpha', 'lowBeta', 'lowGama', 'theta']

for wave in wave_columns:
    # Stack all 180D vectors into one long 1D array
    wave_data = df[wave].apply(np.array).to_list()  # turns each row into a numpy array
    wave_data = np.concatenate(wave_data)  # flatten to shape (n_patients * 180,)

    # Normality tests
    stat_shapiro, p_shapiro = shapiro(wave_data)  # Shapiro limit is ~5000 samples
    stat_dagostino, p_dagostino = normaltest(wave_data)

    print(f"\nWave: {wave}")
    print(f"Shapiro-Wilk p = {p_shapiro:.4f} → {'Normal' if p_shapiro > 0.05 else 'Not Normal'}")
    print(f"D’Agostino p = {p_dagostino:.4f} → {'Normal' if p_dagostino > 0.05 else 'Not Normal'}")

    # Plot histogram + KDE
    plt.figure(figsize=(6, 4))
    sns.histplot(wave_data, kde=True, bins=40, color='teal')
    plt.title(f'Distribution of {wave} (Shapiro p={p_shapiro:.4f})')
    plt.xlabel(f'{wave} value')
    plt.ylabel('Frequency')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

import torch
import os
import numpy as np
import pandas as pd
df = pd.read_csv('data1.csv')

import numpy as np

#data preprocessing
import ast
wave_columns = ['delta','highAlpha', 'highBeta',
'lowAlpha', 'lowBeta', 'lowGama', 'theta']

df = df.drop(index=77).reset_index(drop=True)
def parse_list_of_floats(cell):
    try:
        values = ast.literal_eval(cell)
        return [float(x) for x in values]
    except Exception:
        return [np.nan] * 179  # ensures consistent shape

# Step 5: Apply parsing and expand each EEG feature
for col in wave_columns:
    df[col] = df[col].apply(parse_list_of_floats)


eeg_tensor = np.stack([np.stack(df[col].values) for col in wave_columns], axis=1).astype(np.float32)
print("Before normalization:", eeg_tensor.shape)

# Step 6: Normalize per-sample across each channel (along axis=2)

for i, col in enumerate(wave_columns):
    df[col] = list(eeg_tensor[:, i, :])

def log_z_normalize_column(col_data):
    """Apply log1p followed by z-score to each 180D vector."""
    return col_data.apply(lambda x: (
        (np.log1p(np.array(x, dtype=np.float32)) -
         np.log1p(np.array(x, dtype=np.float32)).mean()) /
         np.log1p(np.array(x, dtype=np.float32)).std()
    ))
# Step 3: Apply to each wave column
for col in wave_columns:
    df[col] = log_z_normalize_column(df[col])
    print(df[col].dtype)

#eeg_min = eeg_tensor.min(axis=2, keepdims=True)
#eeg_max = eeg_tensor.max(axis=2, keepdims=True)
#eeg_tensor = (eeg_tensor - eeg_min) / (eeg_max - eeg_min + 1e-8)

#for i, col in enumerate(wave_columns):
    #df[col] = list(eeg_tensor[:, i, :])

# Extract labels (actual age)
df['user_birth_data'] = pd.to_datetime(df['user_birth_data'])

# Get today's date
today = pd.to_datetime('today')

# Compute age in years
df['actual_age'] = (today - df['user_birth_data']).dt.days // 365
y_age = df['actual_age'].values.astype(np.float32)

y_mean = df['actual_age'].mean()
y_std = df['actual_age'].std()


# Step 7: Final check
print("After normalization:", eeg_tensor.shape)
print("Example tensor (first sample, first channel):", eeg_tensor[0, 0, :5])
print("Target shape:", y_age.shape)
df["highAlpha"].iloc[0].dtype

#create dataset
X = torch.tensor(df[wave_columns].values.tolist()) #shape is [number of patients, 7, 180]
# Extract labels (actual age)
df['user_birth_data'] = pd.to_datetime(df['user_birth_data'])
torch.tensor(X)

# Get today's date
today = pd.to_datetime('today')

# Compute age in years
df['actual_age'] = (today - df['user_birth_data']).dt.days // 365

#normalize ages

y_mean = df['actual_age'].mean()
y_std = df['actual_age'].std()


df['age_normalized'] = (df['actual_age'] - y_mean) / y_std


Y = df['age_normalized'].values.astype(np.float32)

#split data
from sklearn.model_selection import train_test_split

# Split 80% train, 10% val, 10% test

X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.2, random_state=42)
X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)

print(X_train.shape, Y_train.shape)
print(X_val.shape, Y_val.shape)
print(X_test.shape, Y_test.shape)

#print age distribution
import matplotlib.pyplot as plt

# Plot histogram
plt.figure(figsize=(8, 5))
plt.hist(df['actual_age'], bins=20, edgecolor='black', color='skyblue')
plt.title('Age Distribution')
plt.xlabel('Age (years)')
plt.ylabel('Number of Participants')
plt.grid(True)
plt.show()

#create dataloader
from torch.utils.data import TensorDataset, DataLoader

# Wrap in TensorDataset
train_dataset = TensorDataset(X_train, torch.tensor(Y_train))
val_dataset = TensorDataset(X_val, torch.tensor(Y_val))
test_dataset = TensorDataset(X_test, torch.tensor(Y_test))

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)
test_loader = DataLoader(test_dataset, batch_size=32)

print(f"Training samples: {len(train_loader.dataset)}")
print(f"Validation samples: {len(val_loader.dataset)}")
print(f"Test samples: {len(test_loader.dataset)}")

#1D CNN model
import torch
import torch.nn as nn
import torch.nn.functional as F

import torch
import torch.nn as nn
import torch.nn.functional as F

class EEG1DCNN(nn.Module):
    def __init__(self):
        super(EEG1DCNN, self).__init__()

        self.conv1 = nn.Conv1d(in_channels=7, out_channels=16, kernel_size=3, stride=1, padding=2)
        self.bn1 = nn.BatchNorm1d(16)
        self.pool1 = nn.MaxPool1d(kernel_size=2)

        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=2)
        self.bn2 = nn.BatchNorm1d(32)
        self.pool2 = nn.MaxPool1d(kernel_size=2)

        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm1d(64)
        self.pool3 = nn.AdaptiveMaxPool1d(10)  # Output shape: [B, 128, 10]

        self.fc1 = nn.Linear(64 * 10, 32)
        self.dropout = nn.Dropout(0.3) #0.3 is better than 0.2 and 0.5
        self.fc2 = nn.Linear(32, 1)  # Regression output

    def forward(self, x):
        # Input shape: [B, 1, 10, 179]
        x = x.squeeze(1)  # Shape: [B, 10, 179]
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.pool1(x)

        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.pool2(x)

        x = self.conv3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.pool3(x)

        x = x.view(x.size(0), -1)  # Flatten
        x = self.dropout(F.relu(self.fc1(x)))
        x = self.fc2(x)

        return x

#training function
import time
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random


def get_model_name(name, batch_size, learning_rate, epoch):
    return f"model_{name}_bs{batch_size}_lr{learning_rate}_epoch{epoch}"

def train_net(net, train_loader, val_loader, y_mean, y_std, batch_size=64, learning_rate=1e-3, num_epochs=50, checkpoint_freq=5):
    device = torch.device("cpu")
    net.to(device)

    criterion = nn.L1Loss()  # MAE loss on normalized values
    optimizer = optim.Adam(net.parameters(), lr=learning_rate)

    train_loss = np.zeros(num_epochs)
    val_loss = np.zeros(num_epochs)

    print("Starting training...\n")
    start_time = time.time()

    for epoch in range(num_epochs):
        net.train()
        total_train_loss = 0.0
        total_samples = 0

        for inputs, labels in train_loader:
            inputs = inputs.to(device, dtype=torch.float32)
            labels = labels.float().view(-1, 1).to(device)

            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            total_train_loss += loss.item() * labels.size(0)
            total_samples += labels.size(0)

        train_loss[epoch] = total_train_loss / total_samples

        # -------- Validation & Prediction --------
        net.eval()
        val_outputs = []
        val_labels = []

        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs = inputs.to(device, dtype=torch.float32)
                labels = labels.float().view(-1, 1).to(device)

                outputs = net(inputs)

                val_outputs.append(outputs.cpu().numpy())
                val_labels.append(labels.cpu().numpy())

        # Stack and flatten
        pred_norm = np.concatenate(val_outputs).flatten()
        true_norm = np.concatenate(val_labels).flatten()

        # Inverse Z-score to get real ages
        pred_age = pred_norm * y_std + y_mean
        true_age = true_norm * y_std + y_mean

        # MAE in real age units
        real_mae = np.mean(np.abs(pred_age - true_age))
        val_loss[epoch] = real_mae

        # Print info
        print(f"Epoch {epoch+1}: Train Loss (norm) = {train_loss[epoch]:.4f} | Val MAE (age) = {real_mae:.2f} years")
        print("Sample predictions:")
        indices = random.sample(range(len(pred_age)), 5)
        for i in indices:
          print(f"  Predicted: {pred_age[i]:.1f}  |  Actual: {true_age[i]:.1f}")

        # Save checkpoint
        if (epoch + 1) % checkpoint_freq == 0 or epoch == num_epochs - 1:
            torch.save(net.state_dict(), f"model_epoch_{epoch+1}.pt")
            print(f"Checkpoint saved: model_epoch_{epoch+1}.pt\n")

    print(f"\nTraining completed in {time.time() - start_time:.2f} seconds.")
    return train_loss, val_loss

#evaluation function
def evaluate(net, loader, criterion, device):
    net.eval()
    total_loss = 0.0
    total_samples = 0
    with torch.no_grad():
        for inputs, labels in loader:
            inputs = inputs.to(device, dtype=torch.float32)
            labels = labels.to(device).float().view(-1, 1)
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            total_loss += loss.item() * labels.size(0)
            total_samples += labels.size(0)
    avg_loss = total_loss / total_samples
    return avg_loss

#helper function
def get_model_name(name, batch_size, learning_rate, epoch):
    """ Generate a name for the model consisting of all the hyperparameter values

    Args:
        config: Configuration object containing the hyperparameters
    Returns:
        path: A string with the hyperparameter name and value concatenated
    """
    path = "model_{0}_bs{1}_lr{2}_epoch{3}".format(name,
                                                   batch_size,
                                                   learning_rate,
                                                   epoch)
    return path

EEG_net = EEG1DCNN()

train_net(EEG_net, train_loader, val_loader, y_mean, y_std, num_epochs=70, checkpoint_freq=5)
model_path1 = get_model_name("EEG_net", batch_size=32, learning_rate=1e-4, epoch=70)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
evaluate(EEG_net, test_loader, nn.L1Loss(), device=device)

#RNN
import torch
import torch.nn as nn
import torch.nn.functional as F

class EEGRNN(nn.Module):
    def __init__(self, input_size=7, hidden_size=64, num_layers=2, dropout=0.3):
        super(EEGRNN, self).__init__()

        # RNN expects input of shape [batch_size, seq_len, input_size]
        self.rnn = nn.LSTM(input_size=input_size,
                           hidden_size=hidden_size,
                           num_layers=num_layers,
                           batch_first=True,
                           dropout=dropout,
                           bidirectional=False)

        self.fc1 = nn.Linear(hidden_size, 32)
        self.dropout = nn.Dropout(dropout)
        self.fc2 = nn.Linear(32, 1)  # Output: predicted age

    def forward(self, x):
        # x shape: [B, C, T] => [B, T, C]
        x = x.permute(0, 2, 1)

        # RNN output
        out, (hn, cn) = self.rnn(x)  # out: [B, T, hidden], hn: [num_layers, B, hidden]
        last_hidden = hn[-1]         # Take last layer's hidden state: [B, hidden]

        x = F.relu(self.fc1(last_hidden))
        x = self.dropout(x)
        x = self.fc2(x)

        return x

EEG_net = EEGRNN()

train_net(EEG_net, train_loader, val_loader, y_mean, y_std, batch_size=32, learning_rate=5e-3, num_epochs=70, checkpoint_freq=5)
model_path1 = get_model_name("EEG_net", batch_size=64, learning_rate=1e-3, epoch=70)
test_acc = model_test_accuracy(EEG_net, test_loader)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

#transformer
import torch
import torch.nn as nn

class EEGTransformer(nn.Module):
    def __init__(self, input_channels=7, seq_len=179, d_model=64, nhead=4, num_layers=2, dropout=0.3):
        super(EEGTransformer, self).__init__()

        self.input_proj = nn.Linear(input_channels, d_model)  # project channels to d_model
        self.pos_embedding = nn.Parameter(torch.randn(1, seq_len, d_model))

        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(seq_len * d_model, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1)  # Output a single age prediction
        )

    def forward(self, x):
        # x: [B, C, T] → [B, T, C]
        x = x.permute(0, 2, 1)

        x = self.input_proj(x)  # [B, T, d_model]
        x = x + self.pos_embedding  # Add positional encoding

        x = self.transformer_encoder(x)  # [B, T, d_model]

        out = self.fc(x)  # Final regressor
        return out.squeeze()

EEG_net = EEGTransformer()
train_net(EEG_net, train_loader, val_loader, y_mean, y_std, batch_size=64, learning_rate=1e-3, num_epochs=70, checkpoint_freq=5)
model_path1 = get_model_name("EEG_net", batch_size=64, learning_rate=1e-3, epoch=70)

#random forest
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split

# ----- STEP 1: Simulated EEG data loading -----
# Let's assume you already have this from your real dataset:
# X: EEG signals → shape (n_samples, n_channels, n_timepoints)
# Y: Chronological age → shape (n_samples,)

# Example: random simulated EEG for testing (replace this with your real data)
n_samples = 120
n_channels = 7
n_timepoints = 179
np.random.seed(42)
X = np.random.rand(n_samples, n_channels, n_timepoints)
Y = np.random.randint(35, 67, size=(n_samples,))  # age between 35 and 67

# ----- STEP 2: Flatten EEG input for Random Forest -----
# Reshape X to 2D: (n_samples, n_channels * n_timepoints)
X_flat = X.reshape(X.shape[0], -1)

# ----- STEP 3: Train-Test Split -----
X_train, X_test, y_train, y_test = train_test_split(
    X_flat, Y, test_size=0.1, random_state=42
)

# ----- STEP 4: Fit Random Forest Regressor -----
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# ----- STEP 5: Evaluate Model -----
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Test MAE: {mae:.2f} years")
print(f"Test R²: {r2:.3f}")





















































































































































































